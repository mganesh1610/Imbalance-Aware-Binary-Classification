{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from scipy.stats import loguniform\n",
        "import re\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 1. Load Data\n",
        "# ---------------------------------------------------\n",
        "labeled_path = \"https://drive.google.com/uc?id=15WUl8ilQyE2MRXjnO-V2x7rJXNMxI7qI\"\n",
        "unlabeled_path = \"https://drive.google.com/uc?id=1J57A0_EdtHW4yZU1Kus6ymekxM4DzZrp\"\n",
        "\n",
        "df_labeled = (\n",
        "    pd.read_excel(labeled_path, engine=\"openpyxl\")\n",
        "      .rename(columns={\"Unnamed: 0\": \"index\"})\n",
        "      .dropna()\n",
        ")\n",
        "df_unlabeled = (\n",
        "    pd.read_excel(unlabeled_path, engine=\"openpyxl\")\n",
        "      .rename(columns={\"Unnamed: 0\": \"index\"})\n",
        ")\n",
        "\n",
        "index_col, target_col = \"index\", \"label\"\n",
        "X = df_labeled.drop(columns=[index_col, target_col])\n",
        "y = df_labeled[target_col]\n",
        "X_unlabeled = df_unlabeled.drop(columns=[index_col])\n",
        "\n",
        "print(\"Data shape:\", X.shape)\n",
        "print(\"Class distribution:\", np.bincount(y))\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 2. Preprocessing\n",
        "# ---------------------------------------------------\n",
        "cat_cols = [\"x2\", \"x3\", \"x4\"]\n",
        "num_cols = [\"x15\", \"x16\", \"x17\", \"x18\", \"x19\", \"x20\", \"x21\"]\n",
        "bin_cols = [c for c in X.columns if c not in cat_cols + num_cols]\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "        (\"bin\", \"passthrough\", bin_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 3. Balanced Error Rate\n",
        "# ---------------------------------------------------\n",
        "def balanced_error_rate(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        per_class_error = 1.0 - np.diag(cm) / cm.sum(axis=1)\n",
        "    return np.nanmean(per_class_error)\n",
        "\n",
        "ber_scorer = make_scorer(balanced_error_rate, greater_is_better=False)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 4. PROPER NESTED CROSS-VALIDATION\n",
        "# ---------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROPER NESTED CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define pipeline\n",
        "rfe_svm_pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"selector\", RFE(\n",
        "        estimator=SVC(kernel=\"linear\", class_weight=\"balanced\", random_state=42),\n",
        "        step=1\n",
        "    )),\n",
        "    (\"clf\", SVC(\n",
        "        kernel=\"rbf\",\n",
        "        class_weight=\"balanced\",\n",
        "        probability=True,\n",
        "        cache_size=1000,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"selector__n_features_to_select\": [8, 10, 12, 15, 18],\n",
        "    \"clf__C\": loguniform(1e0, 1e3),\n",
        "    \"clf__gamma\": loguniform(1e-4, 1e-1),\n",
        "}\n",
        "\n",
        "# OUTER CV: For final evaluation\n",
        "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "outer_scores = []\n",
        "outer_probas = []\n",
        "outer_best_params = []\n",
        "\n",
        "print(\"Running 5-fold outer CV (each fold has inner 4-fold CV for hyperparam tuning)...\")\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), 1):\n",
        "    print(f\"\\n--- Outer Fold {fold}/5 ---\")\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "    # INNER CV: Hyperparameter tuning on training fold\n",
        "    inner_search = RandomizedSearchCV(\n",
        "        rfe_svm_pipeline,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        scoring=ber_scorer,\n",
        "        cv=4,  # Inner CV\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    inner_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get best model from inner search\n",
        "    best_inner_model = inner_search.best_estimator_\n",
        "    outer_best_params.append(inner_search.best_params_)\n",
        "\n",
        "    # Evaluate on outer test fold\n",
        "    y_pred = best_inner_model.predict(X_test)\n",
        "    fold_score = balanced_error_rate(y_test, y_pred)\n",
        "    outer_scores.append(fold_score)\n",
        "\n",
        "    # Get probabilities for threshold tuning later\n",
        "    if hasattr(best_inner_model, 'predict_proba'):\n",
        "        y_proba = best_inner_model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_proba = y_pred.astype(float)\n",
        "\n",
        "    # Store for overall threshold optimization\n",
        "    for i, idx in enumerate(test_idx):\n",
        "        if idx < len(outer_probas):\n",
        "            outer_probas[idx] = y_proba[i]\n",
        "        else:\n",
        "            # Extend list if needed\n",
        "            outer_probas.extend([np.nan] * (idx - len(outer_probas) + 1))\n",
        "            outer_probas[idx] = y_proba[i]\n",
        "\n",
        "# Convert scores to regular floats for better printing\n",
        "outer_scores_clean = [float(score) for score in outer_scores]\n",
        "print(f\"\\nOuter CV scores: {outer_scores_clean}\")\n",
        "print(f\"Mean Outer CV BER: {np.mean(outer_scores_clean):.4f}\")\n",
        "print(f\"Std Outer CV BER: {np.std(outer_scores_clean):.4f}\")\n",
        "\n",
        "# Convert outer_probas to array (handle any missing)\n",
        "outer_probas_array = np.array([p for p in outer_probas if not np.isnan(p)])\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 5. THRESHOLD OPTIMIZATION ON OUTER CV PROBABILITIES\n",
        "# ---------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"THRESHOLD OPTIMIZATION ON OUTER CV PROBABILITIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "thresholds = np.linspace(0.1, 0.9, 81)\n",
        "best_ber_nested = 1.0\n",
        "best_thresh_nested = 0.5\n",
        "\n",
        "for t in thresholds:\n",
        "    preds = (outer_probas_array >= t).astype(int)\n",
        "    ber = balanced_error_rate(y[:len(outer_probas_array)], preds)\n",
        "    if ber < best_ber_nested:\n",
        "        best_ber_nested = ber\n",
        "        best_thresh_nested = t\n",
        "\n",
        "print(f\"Optimal threshold from NESTED CV: {best_thresh_nested:.3f}\")\n",
        "print(f\"BER at optimal threshold: {best_ber_nested:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 6. FINAL TRAINING WITH BEST HYPERPARAMETERS\n",
        "# ---------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# FIXED: Better approach to find most common parameters\n",
        "# Create a dictionary to track frequency of each parameter combination\n",
        "from collections import defaultdict\n",
        "\n",
        "# We'll round the float parameters to avoid precision issues\n",
        "param_freq = defaultdict(int)\n",
        "\n",
        "for params in outer_best_params:\n",
        "    # Extract and convert parameters\n",
        "    n_features = int(params.get('selector__n_features_to_select', 15))\n",
        "    C_val = float(params.get('clf__C', 10.0))\n",
        "    gamma_val = float(params.get('clf__gamma', 0.01))\n",
        "\n",
        "    # Round to reasonable precision for comparison\n",
        "    C_rounded = round(C_val, 2)\n",
        "    gamma_rounded = round(gamma_val, 6)\n",
        "\n",
        "    # Create a tuple key\n",
        "    key = (n_features, C_rounded, gamma_rounded)\n",
        "    param_freq[key] += 1\n",
        "\n",
        "# Find the most common parameter combination\n",
        "most_common_key = max(param_freq.items(), key=lambda x: x[1])[0]\n",
        "n_features_common, C_common, gamma_common = most_common_key\n",
        "\n",
        "print(f\"Most common parameter combination (across {len(outer_best_params)} folds):\")\n",
        "print(f\"  n_features_to_select: {n_features_common}\")\n",
        "print(f\"  C: {C_common}\")\n",
        "print(f\"  gamma: {gamma_common}\")\n",
        "print(f\"  Frequency: {param_freq[most_common_key]}/{len(outer_best_params)}\")\n",
        "\n",
        "# Alternatively, use median values for continuous parameters\n",
        "n_features_values = [int(p.get('selector__n_features_to_select', 15)) for p in outer_best_params]\n",
        "C_values = [float(p.get('clf__C', 10.0)) for p in outer_best_params]\n",
        "gamma_values = [float(p.get('clf__gamma', 0.01)) for p in outer_best_params]\n",
        "\n",
        "n_features_final = int(np.median(n_features_values))\n",
        "C_final = float(np.median(C_values))\n",
        "gamma_final = float(np.median(gamma_values))\n",
        "\n",
        "print(f\"\\nMedian parameter values (alternative approach):\")\n",
        "print(f\"  n_features_to_select: {n_features_final}\")\n",
        "print(f\"  C: {C_final:.2f}\")\n",
        "print(f\"  gamma: {gamma_final:.6f}\")\n",
        "\n",
        "# Choose which approach to use (here we'll use median)\n",
        "print(f\"\\nUsing MEDIAN parameter values for final model...\")\n",
        "\n",
        "# Train final model on ALL data with these params\n",
        "final_pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"selector\", RFE(\n",
        "        estimator=SVC(kernel=\"linear\", class_weight=\"balanced\", random_state=42),\n",
        "        n_features_to_select=n_features_final,\n",
        "        step=1\n",
        "    )),\n",
        "    (\"clf\", SVC(\n",
        "        kernel=\"rbf\",\n",
        "        class_weight=\"balanced\",\n",
        "        probability=True,\n",
        "        cache_size=1000,\n",
        "        random_state=42,\n",
        "        C=C_final,\n",
        "        gamma=gamma_final\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"Training final model on all labeled data...\")\n",
        "final_pipeline.fit(X, y)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 7. MAKE PREDICTIONS\n",
        "# ---------------------------------------------------\n",
        "print(\"Making predictions on unlabeled data...\")\n",
        "final_proba = final_pipeline.predict_proba(X_unlabeled)[:, 1]\n",
        "final_preds = (final_proba >= best_thresh_nested).astype(int)\n",
        "\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "print(f\"  Class 0: {sum(final_preds == 0)} ({sum(final_preds == 0)/len(final_preds)*100:.1f}%)\")\n",
        "print(f\"  Class 1: {sum(final_preds == 1)} ({sum(final_preds == 1)/len(final_preds)*100:.1f}%)\")\n",
        "\n",
        "# Save predictions\n",
        "filename = \"ProjectPredictions2025_SVM_RFE_Nested_CV.csv\"\n",
        "pred_df = pd.DataFrame({\n",
        "    index_col: df_unlabeled[index_col],\n",
        "    target_col: final_preds\n",
        "})\n",
        "pred_df.to_csv(filename, index=False)\n",
        "\n",
        "print(f\"\\nSaved Nested CV predictions to {filename}\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 8. COMPARE WITH YOUR ORIGINAL RESULTS\n",
        "# ---------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON WITH PREVIOUS RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Your original validation BER:           0.3330 (optimistic)\")\n",
        "print(f\"Your original CV BER (threshold 0.5):  0.3459\")\n",
        "print(f\"Your original CV BER (threshold 0.28): 0.3454\")\n",
        "print(f\"Nested CV BER (proper evaluation):      {best_ber_nested:.4f}\")\n",
        "\n",
        "if best_ber_nested < 0.3454:\n",
        "    print(f\"\\n✓ Nested CV shows TRUE performance: {best_ber_nested:.4f}\")\n",
        "    print(\"  Submit Nested CV predictions\")\n",
        "elif abs(best_ber_nested - 0.3454) < 0.01:\n",
        "    print(f\"\\n≈ Similar results\")\n",
        "    print(\"  Either predictions are fine\")\n",
        "else:\n",
        "    print(f\"\\n⚠ Nested CV gives different result\")\n",
        "    print(f\"  Consider which evaluation is more trustworthy\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 9. ADDITIONAL MODEL INFO (for report)\n",
        "# ---------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL INFORMATION FOR REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: SVM with RBF kernel and RFE feature selection\")\n",
        "print(f\"Preprocessing:\")\n",
        "print(f\"  - Categorical columns (x2, x3, x4): OneHotEncoder\")\n",
        "print(f\"  - Numerical columns (x15-x21): StandardScaler\")\n",
        "print(f\"  - Binary columns: Pass-through\")\n",
        "print(f\"Feature selection: RFE with linear SVM, selecting {n_features_final} features\")\n",
        "print(f\"Hyperparameters:\")\n",
        "print(f\"  - C (regularization): {C_final:.2f}\")\n",
        "print(f\"  - gamma (kernel coefficient): {gamma_final:.6f}\")\n",
        "print(f\"  - Probability threshold: {best_thresh_nested:.3f}\")\n",
        "print(f\"Cross-validation: 5-fold outer CV, 4-fold inner CV\")\n",
        "print(f\"Final BER estimate: {best_ber_nested:.4f}\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T06:45:21.747452Z",
          "iopub.execute_input": "2025-12-03T06:45:21.747835Z",
          "iopub.status.idle": "2025-12-03T08:06:44.632977Z",
          "shell.execute_reply.started": "2025-12-03T06:45:21.747800Z",
          "shell.execute_reply": "2025-12-03T08:06:44.632107Z"
        },
        "id": "GyLQ5rop5IJ2",
        "outputId": "c4000133-7afa-4830-f3dd-a6e9cb3d271a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Data shape: (10000, 21)\nClass distribution: [7503 2497]\n\n============================================================\nPROPER NESTED CROSS-VALIDATION\n============================================================\nRunning 5-fold outer CV (each fold has inner 4-fold CV for hyperparam tuning)...\n\n--- Outer Fold 1/5 ---\n\n--- Outer Fold 2/5 ---\n\n--- Outer Fold 3/5 ---\n\n--- Outer Fold 4/5 ---\n\n--- Outer Fold 5/5 ---\n\nOuter CV scores: [0.3550231709254619, 0.3433589363937736, 0.36064066841210735, 0.334, 0.339]\nMean Outer CV BER: 0.3464\nStd Outer CV BER: 0.0099\n\n============================================================\nTHRESHOLD OPTIMIZATION ON OUTER CV PROBABILITIES\n============================================================\nOptimal threshold from NESTED CV: 0.280\nBER at optimal threshold: 0.3461\n\n============================================================\nFINAL MODEL TRAINING\n============================================================\nMost common parameter combination (across 5 folds):\n  n_features_to_select: 18\n  C: 63.58\n  gamma: 0.013311\n  Frequency: 3/5\n\nMedian parameter values (alternative approach):\n  n_features_to_select: 18\n  C: 63.58\n  gamma: 0.013311\n\nUsing MEDIAN parameter values for final model...\nTraining final model on all labeled data...\nMaking predictions on unlabeled data...\n\nPrediction distribution:\n  Class 0: 6569 (65.7%)\n  Class 1: 3431 (34.3%)\n\nSaved Nested CV predictions to ProjectPredictions2025_SVM_RFE_Nested_CV.csv\n\n============================================================\nCOMPARISON WITH PREVIOUS RESULTS\n============================================================\nYour original validation BER:           0.3330 (optimistic)\nYour original CV BER (threshold 0.5):  0.3459\nYour original CV BER (threshold 0.28): 0.3454\nNested CV BER (proper evaluation):      0.3461\n\n≈ Similar results\n  Either predictions are fine\n============================================================\n\n============================================================\nMODEL INFORMATION FOR REPORT\n============================================================\nModel: SVM with RBF kernel and RFE feature selection\nPreprocessing:\n  - Categorical columns (x2, x3, x4): OneHotEncoder\n  - Numerical columns (x15-x21): StandardScaler\n  - Binary columns: Pass-through\nFeature selection: RFE with linear SVM, selecting 18 features\nHyperparameters:\n  - C (regularization): 63.58\n  - gamma (kernel coefficient): 0.013311\n  - Probability threshold: 0.280\nCross-validation: 5-fold outer CV, 4-fold inner CV\nFinal BER estimate: 0.3461\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}